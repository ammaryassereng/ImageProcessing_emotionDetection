{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from commonfunctions import *\n",
    "from skimage import io\n",
    "from skimage.color import rgba2rgb, rgb2gray\n",
    "from skimage import filters\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "from PHOG import *\n",
    "from Faces import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7944941",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75433ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadFile(path,label,imgs,labels):\n",
    "    files = os.listdir(path)\n",
    "    num_images = len(files)\n",
    "    count = 0\n",
    "    for f in files:\n",
    "        img = io.imread(os.path.join(path, f), as_gray=False)\n",
    "        if len(img.shape) != 2:\n",
    "            img = rgb2gray(img)\n",
    "        # img = cv2.resize(img, dsize=(60, 60), interpolation=cv2.INTER_CUBIC)\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "        count += 1\n",
    "        if count>=100:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReadFile(\"./train/happy\",\"happy\",imgs,labels)\n",
    "ReadFile(\"./train/sad\",\"sad\",imgs,labels)\n",
    "ReadFile(\"./train/neutral\",\"neutral\",imgs,labels)\n",
    "# ReadFile(\"./train/fearful\",\"fear\",imgs,labels)\n",
    "# ReadFile(\"./train/surprised\",\"surprise\",imgs,labels)\n",
    "ReadFile(\"./train/happy\",\"happy\",imgs,labels)\n",
    "ReadFile(\"./train/sad\",\"sad\",imgs,labels)\n",
    "# ReadFile(\"./train/fearful\",\"fear\",imgs,labels)\n",
    "ReadFile(\"./train/neutral\",\"neutral\",imgs,labels)\n",
    "# ReadFile(\"./train/surprised\",\"surprise\",imgs,labels)\n",
    "ReadFile(\"./train/happy\",\"happy\",imgs,labels)\n",
    "ReadFile(\"./train/sad\",\"sad\",imgs,labels)\n",
    "ReadFile(\"./train/neutral\",\"neutral\",imgs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d60472",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(imgs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPQ(img, window = 3): \n",
    "    # img = fftpack.fft2(img)\n",
    "    # img_ft = np.fft.fftshift(img_ft)\n",
    "    # img = np.divide(img, np.sum(img))\n",
    "    lpq_hist = np.zeros((0))\n",
    "    patch_height = patch_width = window\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    block1 = img[0:(w//2), 0:h//2]\n",
    "    block2 = img[(w//2):w, 0:h//2]\n",
    "    block3 = img[0:(w//2), (h//2):h]\n",
    "    block4 = img[(w//2):w, (h//2):h]\n",
    "    block_list = [block1,block2,block3,block4]\n",
    "    mid = window // 2\n",
    "    count = 0\n",
    "    for block in block_list:\n",
    "        block_w, block_h= block.shape\n",
    "        block_hist = np.zeros(256)\n",
    "        for i in range (patch_width , block_w-patch_width):\n",
    "            for j in range (patch_height, block_h - patch_width):\n",
    "                temp = 0\n",
    "                patch = block[i - mid : i + mid + 1 , j - mid : j + mid + 1]\n",
    "                patch = np.fft.fft2(patch)\n",
    "                patch = np.fft.fftshift(patch)\n",
    "                if(np.imag(patch[mid-1][mid]) > 0):\n",
    "                    temp += 1\n",
    "                if(np.real(patch[mid-1][mid])> 0):\n",
    "                    temp += 2\n",
    "                if(np.imag(patch[mid][0])> 0):\n",
    "                    temp += 4\n",
    "                if(np.real(patch[mid][0])> 0):\n",
    "                    temp += 8\n",
    "                if(np.imag(patch[mid][mid-1])> 0):\n",
    "                    temp += 16\n",
    "                if(np.real(patch[mid][mid-1])> 0):\n",
    "                    temp += 32\n",
    "                if(np.imag(patch[mid-2][mid-1])> 0):\n",
    "                    temp += 64\n",
    "                if(np.real(patch[mid-2][mid-1])> 0):\n",
    "                    temp += 128\n",
    "                # lpq_hist.append(temp)\n",
    "                block_hist[temp] += 1\n",
    "        block_hist = block_hist.astype('uint8')\n",
    "        lpq_hist = np.append(lpq_hist,block_hist)\n",
    "    return lpq_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=  0\n",
    "features_3 = []\n",
    "for img in imgs:\n",
    "    phog = PHOG(img, 40, 3)\n",
    "    lpq = LPQ(img,3)\n",
    "    temp = np.concatenate((phog, lpq))\n",
    "    features_3.append(temp)\n",
    "    cnt += 1\n",
    "    if(cnt % 100 == 0):\n",
    "        print(cnt)\n",
    "features_3 = np.array(features_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=  0\n",
    "features_phog = []\n",
    "for img in imgs:\n",
    "    phog = PHOG(img, 40, 3)\n",
    "    features_phog.append(phog)\n",
    "    cnt += 1\n",
    "    if(cnt % 100 == 0):\n",
    "        print(cnt)\n",
    "features_phog = np.array(features_phog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "labels[labels==\"happy\"] = 1\n",
    "labels[labels==\"sad\"] = 2\n",
    "labels[labels==\"angry\"] = 3\n",
    "labels[labels==\"fear\"] = 4\n",
    "labels[labels==\"surprise\"] = 3\n",
    "labels[labels==\"neutral\"] = 5\n",
    "labels = labels.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features_3, labels, test_size=0.2,stratify=labels)\n",
    "\n",
    "# pca = PCA(n_components=110)\n",
    "# x_train = pca.fit_transform(x_train)\n",
    "clf3 = svm.SVC()\n",
    "model = clf3.fit(x_train, y_train)\n",
    "\n",
    "# new = pca.transform(features_2[0].reshape((1, -1)))\n",
    "# x_test = pca.fit_transform(x_test)\n",
    "pred = clf3.predict(x_test)\n",
    "acc = (np.sum(y_test == pred) / len(y_test))*100\n",
    "print(f'Accuracy when using PHOG + LPQ : {acc} %')\n",
    "print(x_test.shape)\n",
    "# target_names = ['happy', 'sad', 'angry', 'fear', 'surprise']\n",
    "# print(classification_report(y_test, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358931dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = []\n",
    "# y_test = []\n",
    "# faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# def getCroppedFaces(faceimg):\n",
    "#     # colorB = np.copy(faceimg[:,:,0])\n",
    "#     # colorR = np.copy(faceimg[:,:,2])\n",
    "#     # faceimg[:,:,0] = colorR\n",
    "#     # faceimg[:,:,2] = colorB\n",
    "#     faces = faceCascade.detectMultiScale(faceimg,minNeighbors=5)\n",
    "#     Cropped_faces = []\n",
    "#     for face in faces:\n",
    "#         Cropped_faces.append(np.copy(faceimg[face[1]:face[1]+face[3],face[0]:face[0]+face[2],:]))\n",
    "#     return Cropped_faces[0]\n",
    "\n",
    "# for i in range(1,2):\n",
    "#     tempimg = io.imread('Images/'+str(i)+'.jpg')\n",
    "#     tempimg = getCroppedFaces(tempimg)\n",
    "#     tempimg = rgb2gray(tempimg)\n",
    "#     tempimg = cv2.resize(tempimg, dsize=(60, 60), interpolation=cv2.INTER_CUBIC)\n",
    "#     phog = PHOG(tempimg, 40, 3)\n",
    "#     lpq = LPQ(tempimg,3)\n",
    "#     temp = np.concatenate((phog, lpq))\n",
    "#     x_test.append(temp)\n",
    "\n",
    "# # show_images(x_test)\n",
    "# y_test = [\"happy\", \"sad\", \"happy\",\"sad\", \"happy\", \"happy\",\"happy\"]\n",
    "# x_test = np.array(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d627e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clf3.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features_phog, labels, test_size=0.2,stratify=labels)\n",
    "\n",
    "# pca = PCA(n_components=150)\n",
    "# features_new = pca.fit(x_train)\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "# new = pca.transform(features_2[0].reshape((1, -1)))\n",
    "pred = clf.predict(x_test)\n",
    "acc = (np.sum(y_test == pred) / len(y_test))*100\n",
    "print(f'Accuracy when using only PHOG: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fe72068d906d7c4461303c438056225354d8f601332c1a76eddbfe03f3a0121"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
